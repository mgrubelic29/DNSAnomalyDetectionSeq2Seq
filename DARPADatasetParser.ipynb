{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DARPADatasetParser.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"USa42jyfiWEG","colab_type":"text"},"source":["# **Detecting Network Anomalies using Machine Learning**\n","## Authors: Matthew Grubelic, Chris Saliby, Anthony Saldana, Luke Turbert, Andrew Rittenhouse\n","\n","## Advisor: Liberty Page, Vahid Behzadan, Ph.D.\n","\n","### *Sponsor: Secure and Assured Intelligent Learning (SAIL) Lab*\n","\n","  The risk of crippling cyber attacks on computer systems is increasing rapidly each day. Current software and techniques used to defend against these malicious attacks are showing their limitations and are being completely overwhelmed in some cases. As a result, a more modern and forwardthinking solution is becoming increasingly necessary. The team is implementing one such software solution using a sequence to sequence neural network in Python3 with the Pytorch library to observe malicious events and predict when the next attack might happen. In this project, a deep learning sequence to sequence model, modeled after behavior of predictive typing technologies, was implemented on the CICIDS 2017 dataset to detect malicious DNS traffic and determine the probability of another attack in the future. The model functions by observing sequences of network packets, using them to predict upcoming sequences of packets, and comparing the actual observed data to the prediction. Since the amount of notable research and experimentation done in this area so far is lacking, the results yielded by this network traffic anomaly detection approach further demonstrate that the use of a sequence to sequence model is a viable, though still emerging, solution for modern intrusion detection systems.\n","\n","  Goal: To leverage machine learning techniques to automatically detect anomalous traffic using the DNS protocol. The project seeks to eliminate the need for manual examination of suspicious log files by allowing a machine learning algorithm to make decisions. The machine learning algorithm will use an unsurpervised learning model that is trained off of known data and then compares this data to unknown, live DNS data."]},{"cell_type":"markdown","metadata":{"id":"SZMjhePPmnU8","colab_type":"text"},"source":["  Google has its own API library that allows us to search our local machines for the file that we would like to use and upload for this program. Unfortunately, google only allows us to have the file uploaded for 24 hours until the system is flushed and reset. The following code allows us to search for the specific file that was referenced later on in the program and upload it for use. The for loop in this section will display the name and the bytes of the file so that the user of the program knows that the file has uploaded successfully. Depending on the file, this does take 5-10 minutes to upload which is shown by a percentage to inform us on the progress."]},{"cell_type":"code","metadata":{"id":"6y_wJQOV8rgH","colab_type":"code","outputId":"b5f472c4-1870-4cb8-fecf-18a2e4e742f4","executionInfo":{"status":"error","timestamp":1582653548829,"user_tz":300,"elapsed":25,"user":{"displayName":"Sail Lab","photoUrl":"","userId":"02023648980414798361"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(name = fn, length = len(uploaded[fn])))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m<ipython-input-2-2a6abe5e2837>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0muploaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"]}]},{"cell_type":"markdown","metadata":{"id":"9V30j2Wnn3c5","colab_type":"text"},"source":["  This one liner is not needed for proper exectution of the program, although, it allows the user to see if the file that was uploaded in the above code snippet was uploaded properly. If the file was uploaded properly, then running this code will allow you to see the file name displayed in the current diretory. The file name shown will match the file name that was shown above."]},{"cell_type":"code","metadata":{"id":"VkQQia3IupLi","colab_type":"code","outputId":"ebb2b30c-9da8-44cc-efa1-4506c33f18fd","executionInfo":{"status":"ok","timestamp":1582651114619,"user_tz":300,"elapsed":1163,"user":{"displayName":"Sail Lab","photoUrl":"","userId":"02023648980414798361"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["DARPA1999_week1mon  sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QNdJPGDROgt3","colab_type":"text"},"source":["  Continuing on to the first major piece of the program code, the following snippet mainly processes the data and shows defines a function that will be used later in main(). The beginning of this snippet reads in the data from the previously uploaded file and reads in the file row by row. This allows us to extract the most important bits of information that is held inside the CSV file. After the information is completely read in, the column that displays the protocol type is then filtered to only produce the 'TCP' protocols and insert them into their own array. This will later be a filter that is included so the user can look for any protocol that is present in the CSV file.\n","\n","Moving on to the newly defined function, this function takes in two values. The first is the position of where we are going to start looking in the previously defined protocol array. The second is the DataFrame that we want to look at. The reason this is a DataFrame is because, DataFrames are the most useful and take up the smallest amount of memory. Using regular arrays led to Random Access Memory (RAM) overloads, Google runtime resets, and on a worst case scenario, computer resets. The DataFrames are also useful because they allow you to label the rows and columns of the information that is being processed. Continuing through the function, there are a set of rules that have been added to keep the function running properly and to make sure that we obtain the correct information. The rules are as follows: \n","\n","1. If the Source IP does not match the Source IP of the first packet in the sequence, the function will break\n","2. If the Destination IP does not match the Destination IP of the first packet in the sequence, the function will break\n","3. If the length of the sequences are less than 3 packets total, they will not be included into the final DataFrame. This is because the small number of packets will not be sufficent enough to produce an accurate prediction of the sequences to follow.\n","4. If the current packet did not take place within 1 second of the first packet in the sequence, the function will break.\n","\n","These four rules are the most important for the successful completion of the program. Once these requirements are met, then the function will start generating the sequences that are present in the data set and put them into a  labeled DataFrame. The counter on the last line will then keep track of the number of packets that are present in each sequence that it encounters. This will be helpful later in predicting future sequences. The more packets that are present in a sequence, the more accurate the prediction will be. Finally, this function will return a value - the number of packets present in the sequence and a DataFrame - the labeled sequence."]},{"cell_type":"code","metadata":{"id":"wGRMFQE86Xhe","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import time\n","import os \n","import psutil\n","\n","pd.set_option('display.width', 1000)\n","pd.set_option('display.max_columns', None)\n","\n","# DATA PREPROCESSING------------------------------------------------------------\n","data = pd.read_csv(\"DARPA1999_week1mon\", delimiter=',', header=0, quotechar='\"')\n","tcp_data = data.loc[data['Protocol']=='TCP']\n","\n","# Gets the next input sequence of packets\n","def get_sequence(pos, input_data):\n","    t1 = input_data[pos][1]  # Timestamp of the first packet in the sequence\n","    src1 = input_data[pos][2]  # Source IP of the first packet in the sequence\n","    num_packets = 0  # Store the number of packets in the sequence\n","    sequence = []\n","\n","    for x in range(pos, pos + 320):\n","        # if packet does not have the same source or destination IP as the source of the first one in the sequence\n","        if (input_data[x][2] != src1 and input_data[x][3] != src1 and len(sequence)>3):\n","            break\n","        # if the current one did not happen within 1 second of the first one in the sequence\n","        elif ((input_data[x][1]-t1)>1 and len(sequence)>3):\n","            break\n","        else:\n","            row = input_data[x]\n","            sequence.append(row)  # Append the sequence with the current packet\n","            num_packets = num_packets + 1  # Update the number of packets\n","\n","    return num_packets, sequence"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYgex8wiRKlD","colab_type":"text"},"source":["The following snippet of code is the main part of the program. This is where the function described above will be used and where the filtered dataset that was previously read in will be split between train and test data. The train data is the data that we will use to model the program with. Then the test data will be the part of the data that we will use to test the prediction functions. Furthermore, after the split between the train and test data, the train data is than made into an np.array - an array that contains any object - which we are able to use in the DataFrame. Our DataFrame as of right now is comprised of 8 total columns:\n","1. Packet Number (No.)\n","2. Time\n","3. Source IP\n","4. Destination IP\n","5. Protocol\n","6. Length\n","7. Source Port\n","8. Destination Port\n","\n","These 8 columns are currently being used as the labels in the DataFrame which we will either add or take away from as we feel necessary. Continuing into the while loop, this is where the program starts working. The rules for this loop are as follows:\n","1. If the counting variable exceeds the length of the filtered train data array, the loop will be broken.\n","2. If the length of the filtered train data array subtracted by the count is greater than 3, the loop will break. \n","\n","Rule 2 happens due to the fact that we took away any sequences that are smaller than three packets. This means it is possible that the count will reach a number smaller than the length of the filtered train data array but have no more data to run through. This rule avoids the possibility of the loop going out of bounds. Furthermore, after each sequence is found, the loop will inform the user how many packets are inside the sequence, it will print the sequence to the screen for the user to see, and will also inform the user on the number of sequences it has processed. After all sequences have been accounted for and the loop breaks, the program will then print the time of execution in seconds and end."]},{"cell_type":"code","metadata":{"id":"dsvoL4RlOPKe","colab_type":"code","colab":{}},"source":["# Split into train, test data\n","train, test = train_test_split(tcp_data, test_size=0.33, random_state=0, shuffle=False)\n","print(\"Train: \\n\", train)\n","np_data = np.array(train)\n","\n","# Get all of the sequences\n","count = 1  # Store the total number of processed packets\n","\n","num_sequences = 0  # Store the total number of sequences\n","\n","all_sequences_df = pd.DataFrame(\n","    columns=[\"No.\", \"Time\", \"Source IP\", \"Destination IP\", \"Protocol\", \"Length\", \"Source Port\", \"Destination Port\"])\n","\n","start_time = time.time()\n","\n","while (count < len(np_data) and len(np_data)-count>3):\n","    num_packets, sequence = get_sequence(count, np_data)  # Get the next sequence\n","    count = count + num_packets  # Update the number of processed packets\n","\n","    sequence_df = pd.DataFrame(data=sequence, columns = [\"No.\", \"Time\", \"Source IP\", \"Destination IP\", \"Protocol\", \"Length\", \"Source Port\", \"Destination Port\"])\n","    #print('Number of packets in sequence: ', num_packets)\n","    #print(\"Sequence: \\n\", sequence_df)\n","    all_sequences_df = all_sequences_df.append(sequence_df)\n","\n","    num_sequences = num_sequences + 1  # Update the number of sequences\n","    print(\"Sequence Number: \", num_sequences)\n","    print(\"=========================================================\\n\")\n","\n","print(\"\\n\\n\\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n","print(\"-----> FINISHED READING SEQUENCES <-----\")\n","print(\"Time of Execution: %s seconds\" % (time.time() - start_time))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vq8fMebf17Zs","colab_type":"text"},"source":["After all of the above steps have taken place, the following code snippet saves the DataFrame which will then be saved to the system for the user to have an accessible copy."]},{"cell_type":"code","metadata":{"id":"i4HS1mptmfwE","colab_type":"code","colab":{}},"source":["#print(all_sequences_df)\n","\n","#convert dataframe to csv file \n","#sequenceCSV = all_sequences_df.to_csv(\"/content/SequenceCSV.csv\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OzmXC1zAliUf","colab_type":"text"},"source":["The following few segments of code define the two parts of the sequence to sequence (seq2seq) model: the encoder and the decoder. Each of these two parts is a recurrent neural network (RNN), which is a neural network that performs some operation on a sequence of data and uses the output generated by that operation as input for the next step (recurrence). For these RNNs, we use the **Gated Recurrent Unit** (GRU) architecture, as opposed to the more commonly used **Long Short Term Memory** (LSTM) architecture. This is because, despite being a newer architecture, GRU works similarly to LSTM and has been shown to yield similar results while being slightly more efficient computationally. [This paper](https://arxiv.org/pdf/1412.3555v1.pdf) gives a more in-depth overview of the differences between the two architectures.\n","# Encoder\n","In a seq2seq model using an encoder and decoder, the responsibility of the encoder is to encode, or condense, the input sequence into a single vector while retaining the original meaning of that sequence. For each packet in the input sequence, the encoder will produce two things using the embedding layer:\n","\n","\n","\n","1.   A **vector** (called output_vector in the following code)\n","2.   A **hidden state** (called hidden_state in the following code)\n","\n","\n","\n","Following this, the vector and hidden state will be taken as input to do the next step on the next packet in the sequence, and the output vector will be adjusted accordingly and a new hidden state produced. This process is repeated until a final output vector (the **context vector**) is reached, which will be given to the decoder later on. The forward function carries out these tasks in our implementation."]},{"cell_type":"code","metadata":{"id":"BkW1aaqWlirr","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Recurrent neural network for Encoder of the seq2seq model\n","class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(Encoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size) # Embedding layer\n","        self.gru = nn.GRU(hidden_size, hidden_size) # Applies Gated Recurrent Unit (GRU) to input sequence\n","\n","    def forward(self, input_token, hidden_state):\n","        embedded = self.embedding(input_token).view(1, 1, -1)\n","        output_vector = embedded\n","        output_vector, hidden_state = self.gru(output_vector, hidden_state)\n","        return output_vector, hidden_state\n","\n","    def init_hidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGLjZMy7EkdO","colab_type":"text"},"source":["# Decoder\n","\n","The decoder, like the encoder, is a recurrent neural network using GRU architecture. The decoder takes the context vector as its initial hidden state. As before, the forward function carries out the necessary steps, taking an input token and hidden state as input, then producing an output vector and new hidden state. Unlike the encoder, however, the decoder applies the softmax function to the output vector for normalization.\n","\n"]},{"cell_type":"code","metadata":{"id":"wSPc6zxrvYIl","colab_type":"code","colab":{}},"source":["# Recurrent neural network for Decoder of seq2seq model\n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(Decoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(output_size, hidden_size) # Embedding layer\n","        self.gru = nn.GRU(hidden_size, hidden_size) # Applies GRU\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input_token, hidden_state):\n","        output_vector = self.embedding(input_token).view(1, 1, -1)\n","        output_vector = F.relu(output_vector)\n","        output_vector, hidden_state = self.gru(output_vector, hidden_state)\n","        output_vector = self.softmax(self.out(output_vector[0]))\n","        return output_vector, hidden_state\n","\n","    def init_hidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)\n"],"execution_count":0,"outputs":[]}]}