{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2SeqTraining.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5jV6KTIm0_EU","colab_type":"text"},"source":["This first bit of code takes care of a few things. First, it gets some of the necessary imports out of the way and sets pandas print options. Then, it reads in the sequence csv file and filters out unnecessary columns, leaving 16 total features:\n","\n","1.   The position of the packet within its sequence. This is used only to parse the csv file back out into sequences.\n","2.   The timestamp of the packet\n","3.   The packet length\n","4.   The source port\n","5.   The destination port\n","6.   The 10 possible TCP flags"]},{"cell_type":"code","metadata":{"id":"wIrung1NzTb7","colab_type":"code","outputId":"80bbfe24-51f7-4455-f5af-903543688862","executionInfo":{"status":"ok","timestamp":1588819343989,"user_tz":240,"elapsed":1338,"user":{"displayName":"Sail Lab","photoUrl":"","userId":"02023648980414798361"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["import pandas as pd\n","import numpy as np\n","import time\n","import os\n","import random\n","import psutil\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","pd.set_option('display.width', 1000)\n","pd.set_option('display.max_columns', None)\n","\n","# Read the data and filter out unnecessary columns\n","data = pd.read_csv(\"week1mon_seqs.csv\", delimiter=',', header=0)\n","data = data.filter(items=['position','timestamp','protocol','length','src_port','dst_port',\n","                          'res', 'ns', 'cwr', 'ecn', 'urg', 'ack', 'push', 'reset', 'syn', 'fin'])\n","print(data)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["        position     timestamp protocol  length  src_port  dst_port  res  ns  cwr  ecn  urg  ack  push  reset  syn  fin\n","0              0     37.294817      TCP      60        79      1024    0   0    0    0    0    1     0      0    1    0\n","1              1     37.295017      TCP      60      1024        79    0   0    0    0    0    1     0      0    0    0\n","2              2     37.295563      TCP      60      1024        79    0   0    0    0    0    1     1      0    0    0\n","3              3     37.307251      TCP      60        79      1024    0   0    0    0    0    1     0      0    0    0\n","4              4     37.327150      TCP      60        79      1024    0   0    0    0    0    1     0      0    0    0\n","...          ...           ...      ...     ...       ...       ...  ...  ..  ...  ...  ...  ...   ...    ...  ...  ...\n","826561         3  25638.793159   TELNET     922        23     13416    0   0    0    0    0    1     1      0    0    0\n","826562         0  25638.796071      TCP      60     15037        23    0   0    0    0    0    1     0      0    0    0\n","826563         1  25638.796307   TELNET      60     17258        23    0   0    0    0    0    1     1      0    0    0\n","826564         2  25638.796780   TELNET      60        23     17258    0   0    0    0    0    1     1      0    0    0\n","826565         3  25638.811022      TCP      60     13416        23    0   0    0    0    0    1     0      0    0    0\n","\n","[826566 rows x 16 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fc5l3Xmm1Mbw","colab_type":"text"},"source":["# Feature vector encoding\n","\n","This cell defines two functions to normalize and encode the various features of the dataset. The first function encodes all of the values of a numerical feature as zscores, while the second encodes a text-based feature into a 1 or 0 value that can be interpreted by the model. "]},{"cell_type":"code","metadata":{"id":"FmutqJy3zZpK","colab_type":"code","outputId":"02e3c33c-f1a8-4a7f-aa61-6527443583d4","executionInfo":{"status":"ok","timestamp":1588819344838,"user_tz":240,"elapsed":2170,"user":{"displayName":"Sail Lab","photoUrl":"","userId":"02023648980414798361"}},"colab":{"base_uri":"https://localhost:8080/","height":683}},"source":["# Encode a numeric feature\n","def encode_numeric_zscore(df, name, mean=None, sd=None):\n","    if mean is None:\n","        mean = df[name].mean()\n","\n","    if sd is None:\n","        sd = df[name].std()\n","\n","    df[name] = (df[name] - mean) / sd\n","\n","# Encode a categorical feature\n","def encode_text_dummy(df, name):\n","    dummies = pd.get_dummies(df[name])\n","    for x in dummies.columns:\n","        dummy_name = f\"{name}-{x}\"\n","        df[dummy_name] = dummies[x]\n","    df.drop(name, axis=1, inplace=True)\n","\n","data['src_port_type'] = \"\"\n","data['dst_port_type'] = \"\"\n","data['src_port_type'][data['src_port']<1024] = 'well_known'\n","data['src_port_type'][data['src_port']>49151] = 'dynamic'\n","data['src_port_type'][data['src_port_type'] == \"\"] = 'registered'\n","data['dst_port_type'][data['dst_port']<1024] = 'well_known'\n","data['dst_port_type'][data['dst_port']>49151] = 'dynamic'\n","data['dst_port_type'][data['dst_port_type'] == \"\"] = 'registered'\n","data = data.drop(['src_port'], axis=1)\n","data = data.drop(['dst_port'], axis=1)\n","\n","encode_numeric_zscore(data, \"timestamp\")\n","encode_numeric_zscore(data, \"length\")\n","encode_text_dummy(data, \"protocol\")\n","encode_text_dummy(data, \"src_port_type\")\n","data['src_port_type-dynamic'] = 0\n","encode_text_dummy(data, \"dst_port_type\")\n","data['dst_port_type-dynamic'] = 0\n","\n","data.dropna(inplace=True,axis=1)\n","print(data)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["        position  timestamp    length  res  ns  cwr  ecn  urg  ack  push  reset  syn  fin  protocol-HTTP  protocol-IRC  protocol-POP  protocol-SMTP  protocol-SSHv1  protocol-TCP  protocol-TELNET  src_port_type-registered  src_port_type-well_known  src_port_type-dynamic  dst_port_type-registered  dst_port_type-well_known  dst_port_type-dynamic\n","0              0  -2.120420 -0.393051    0   0    0    0    0    1     0      0    1    0              0             0             0              0               0             1                0                         0                         1                      0                         1                         0                      0\n","1              1  -2.120420 -0.393051    0   0    0    0    0    1     0      0    0    0              0             0             0              0               0             1                0                         1                         0                      0                         0                         1                      0\n","2              2  -2.120420 -0.393051    0   0    0    0    0    1     1      0    0    0              0             0             0              0               0             1                0                         1                         0                      0                         0                         1                      0\n","3              3  -2.120418 -0.393051    0   0    0    0    0    1     0      0    0    0              0             0             0              0               0             1                0                         0                         1                      0                         1                         0                      0\n","4              4  -2.120415 -0.393051    0   0    0    0    0    1     0      0    0    0              0             0             0              0               0             1                0                         0                         1                      0                         1                         0                      0\n","...          ...        ...       ...  ...  ..  ...  ...  ...  ...   ...    ...  ...  ...            ...           ...           ...            ...             ...           ...              ...                       ...                       ...                    ...                       ...                       ...                    ...\n","826561         3   1.855167  1.583610    0   0    0    0    0    1     1      0    0    0              0             0             0              0               0             0                1                         0                         1                      0                         1                         0                      0\n","826562         0   1.855167 -0.393051    0   0    0    0    0    1     0      0    0    0              0             0             0              0               0             1                0                         1                         0                      0                         0                         1                      0\n","826563         1   1.855167 -0.393051    0   0    0    0    0    1     1      0    0    0              0             0             0              0               0             0                1                         1                         0                      0                         0                         1                      0\n","826564         2   1.855167 -0.393051    0   0    0    0    0    1     1      0    0    0              0             0             0              0               0             0                1                         0                         1                      0                         1                         0                      0\n","826565         3   1.855169 -0.393051    0   0    0    0    0    1     0      0    0    0              0             0             0              0               0             1                0                         1                         0                      0                         0                         1                      0\n","\n","[826566 rows x 26 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RZW7FBND1aWY","colab_type":"text"},"source":["# Data preparation\n","\n","This cell begins preparing the data for insertion into the sequence to sequence (seq2seq) model by parsing it into sequences of packets and creating [input, target] pairs. The first for-loop here goes through the dataset and finds the indexes of the start of each sequence and saves them in the \"points\" list. Therefore, the DataFrame rows with indexes that fall between two of these points are considered a single \"sequence\" whose length can range from 4 to 320 total packets.\n","\n","The second for-loop goes through the list of points and uses it to create input and target sequence pairs. The main part of this is handled within a nested for-loop, which reads the data between two break points and stores the first 3 packets as the input and the rest of the sequence as the target. Sequence pairs are stored in this way because the seq2seq model aims to take the first 3 packets of a sequence as input and use them to predict the entire rest of that sequence. While the number of packets in the input remains constant at 3, the number of packets in the target ranges from 1 to 317.\n","\n","To check that everything has worked correctly, a random [input, target] pair is printed along with the total number of detected sequences."]},{"cell_type":"code","metadata":{"id":"keIjrBrfzcIB","colab_type":"code","outputId":"5c2c30f1-aa19-47d9-acf3-72c147f10857","executionInfo":{"status":"ok","timestamp":1588819728328,"user_tz":240,"elapsed":385641,"user":{"displayName":"Sail Lab","photoUrl":"","userId":"02023648980414798361"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# Get the starting points of each sequence within the dataset\n","points = []\n","for x in range(0, len(data.index)):\n","  if(data['position'][x] == 0):\n","    points.append(x)\n","\n","data = data.drop(['position'], axis=1)\n","# Get the input and target sequences for the pairs\n","pairs = []\n","for x in range(0, len(points)-1):\n","  count = 0\n","  input = []\n","  target = []\n","  for y in range(points[x], points[x+1]):\n","    if(count < 3):\n","      # Get input sequence\n","      packet = [data['timestamp'][y], data['length'][y], data['res'][y], data['ns'][y], \n","                data['cwr'][y], data['ecn'][y], data['urg'][y], data['ack'][y], \n","                data['push'][y], data['reset'][y], data['syn'][y], data['fin'][y],\n","                data['protocol-TCP'][y], data['protocol-TELNET'][y], data['protocol-SMTP'][y], \n","                data['protocol-HTTP'][y], data['protocol-SSHv1'][y],data['protocol-IRC'][y], \n","                data['protocol-POP'][y], data['src_port_type-well_known'][y], \n","                data['src_port_type-registered'][y], data['src_port_type-dynamic'][y],\n","                data['dst_port_type-well_known'][y], data['dst_port_type-registered'][y],\n","                data['dst_port_type-dynamic'][y]]\n","      input.append(packet)\n","      count = count + 1\n","    else:\n","      # Get target sequence\n","      packet = [data['timestamp'][y], data['length'][y], data['res'][y], data['ns'][y], \n","                data['cwr'][y], data['ecn'][y], data['urg'][y], data['ack'][y], \n","                data['push'][y], data['reset'][y], data['syn'][y], data['fin'][y],\n","                data['protocol-TCP'][y], data['protocol-TELNET'][y], data['protocol-SMTP'][y], \n","                data['protocol-HTTP'][y], data['protocol-SSHv1'][y],data['protocol-IRC'][y], \n","                data['protocol-POP'][y], data['src_port_type-well_known'][y], \n","                data['src_port_type-registered'][y], data['src_port_type-dynamic'][y],\n","                data['dst_port_type-well_known'][y], data['dst_port_type-registered'][y],\n","                data['dst_port_type-dynamic'][y]]\n","      target.append(packet)\n","      count = count + 1\n","\n","  pair = [input, target]\n","  pairs.append(pair)\n","\n","print(random.choice(pairs))\n","print(len(pairs))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[[0.48494809173918346, -0.39305105735673224, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0.4849500025490883, -0.39305105735673224, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0.48495008966524905, -0.39305105735673224, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]], [[0.4849502213488567, -0.39305105735673224, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]]]\n","81895\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U08BiNbs1a_o","colab_type":"text"},"source":["# Make the tensors\n","\n","The final step of data preparation requires us to transform the [input, target] pairs into tensors so that the model can read them. This is done simply with the following two functions:\n","\n","1.   tensor_from_sequence - converts the token vectors into tensors\n","2.   tensors_from_pair - calls the previous function for the input and target sequences of a given pair and returns the resulting tensors"]},{"cell_type":"code","metadata":{"id":"JlsfTdVmzfkv","colab_type":"code","colab":{}},"source":["# Functions to prepare the data for insertion into the model\n","def tensor_from_sequence(sequence):  # Create a tensor from a sequence using the dictionary\n","    return torch.tensor(sequence, dtype=torch.float, device=device)\n","\n","def tensors_from_pair(pair):  # Get an input and target tensor out of a pair\n","    input_tensor = tensor_from_sequence(pair[0])\n","    target_tensor = tensor_from_sequence(pair[1])\n","    return (input_tensor, target_tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ffMuxHG1blK","colab_type":"text"},"source":["The following few segments of code define the two parts of the actual seq2seq model itself: the encoder and the decoder. Each of these two parts is a recurrent neural network (RNN), which is a neural network that performs some operation on a sequence of data and uses the output generated by that operation as input for the next step (recurrence). For these RNNs, we use the **Gated Recurrent Unit** (GRU) architecture, as opposed to the more commonly used **Long Short Term Memory** (LSTM) architecture. This is because, despite being a newer architecture, GRU works similarly to LSTM and has been shown to yield similar results while being slightly more efficient computationally. [This paper](https://arxiv.org/pdf/1412.3555v1.pdf) gives a more in-depth overview of the differences between the two architectures.\n","# Encoder\n","In a seq2seq model using an encoder and decoder, the responsibility of the encoder is to encode, or condense, the input sequence into a single vector while retaining the original meaning of that sequence. Upon creation, the encoder takes two parameters called input_size and hidden_size, where input_size is the number of features to be used for each token in a given sequence, and hidden_size is the number of features to be used for the hidden state. For each packet in the input sequence, the encoder will produce two things:\n","\n","\n","\n","1.   A **vector** (called output_vector in the following code)\n","2.   A **hidden state** (called hidden_state in the following code)\n","\n","\n","\n","Following this, a new input sequence and the previous hidden state will be taken as input to do the next step on the next packet in the sequence, and the output vector will be adjusted accordingly and a new hidden state produced. This process is repeated until a final output vector (the **context vector**) is reached, which will be given to the decoder later on. The forward function carries out these tasks in our implementation."]},{"cell_type":"code","metadata":{"id":"rCf_629NzmAK","colab_type":"code","colab":{}},"source":["# Recurrent neural network for Encoder of the seq2seq model\n","class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(Encoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size, hidden_size) # Applies Gated Recurrent Unit (GRU) to input sequence\n","\n","    def forward(self, input_token, hidden_state):\n","        input_token = input_token.unsqueeze(0).unsqueeze(0)\n","        output_vector, hidden_state = self.gru(input_token, hidden_state)\n","        return output_vector, hidden_state\n","\n","    def init_hidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0DUMFW1e1cIz","colab_type":"text"},"source":["# Decoder\n","\n","The decoder, like the encoder, is a recurrent neural network using GRU architecture and takes a number of features and hidden size as parameters. However, the number of features for the decoder does not necessarily have to be equal to the number of features for the encoder. For instance, we could theoretically use the decoder to predict the source and destination IPs of packets (2 features), while passing input sequences with only source IPs (1 feature) to the encoder. In this implementation, however, this is not necessary, so the encoder and decoder take an equal number of features.\n","\n","The decoder takes the context vector as its initial hidden state, and has a couple \"extra\" layers compared to the encoder, including the LeakyReLU activation layer. As before, the forward function carries out the necessary steps, taking an input token and hidden state as input, then producing an output vector and new hidden state. The output of each run of the decoder will be a single predicted packet within a larger sequence."]},{"cell_type":"code","metadata":{"id":"iRdRXOo-zpXU","colab_type":"code","colab":{}},"source":["# Recurrent neural network for Decoder of seq2seq model\n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(Decoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(output_size, hidden_size) # Applies GRU\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.relu = nn.LeakyReLU()\n","\n","    def forward(self, input_token, hidden_state):\n","        input_token = input_token.unsqueeze(0).unsqueeze(0)\n","        output_vector = self.relu(input_token)\n","        output_vector, hidden_state = self.gru(output_vector, hidden_state)\n","        output_vector = self.out(output_vector[0])\n","        return output_vector, hidden_state\n","\n","    def init_hidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HC65KhId1cut","colab_type":"text"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"rhEs_wsKzs9P","colab_type":"code","colab":{}},"source":["import math\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","# Helper functions to keep track of the time elapsed and time remaining\n","def as_minutes(sec):\n","    mins = math.floor(sec / 60)\n","    sec = sec - (mins * 60)\n","    return '%dm %ds' % (mins, sec)\n","\n","def time_since(since, percent):\n","    now = time.time()\n","    sec = now-since\n","    es = sec/(percent)\n","    rs = es-sec\n","    return '%sec (- %sec)' % (as_minutes(sec), as_minutes(rs))\n","\n","# Plot loss vs number of iterations\n","def show_plot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    loc = ticker.MultipleLocator(base=0.1) # Put plot ticks at intervals of 0.1\n","    ax.yaxis.set_major_locator(loc)\n","    plt.title(\"Training - Loss vs Num of Iterations\")\n","    plt.xlabel(\"Number of iterations\")\n","    plt.ylabel(\"Loss value\")\n","    plt.plot(points)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JutH17sC1dwT","colab_type":"text"},"source":["# Training function\n","The training function performs the core aspect of machine learning model optimization. In this implementation, it takes several parameters:\n","\n","1. Input tensor which has the tensor representation of the first 3 packets in the sequence\n","2. Target tensor\n","3. The encoder and decoder that the function is going to be training\n","4. Optimizers for the encoder and decoder to oversee the machine learning process\n","5. A function (Mean Squared Error loss) to calculate the training loss, which is called \"criterion\" in the code\n","\n","The function follows a process of several steps, beginning with preparation, which includes initializing the hidden state of the encoder, wiping the gradients of the optimizers, and getting the lengths of the input and target sequences. Next, we have to run the encoder on all of the tokens in the input sequence, which is handled in a loop. Once the encoder has produced a context vector, it is passed to the decoder along with the decoder's input token – the most recently predicted token if teacher forcing is not being used, and the actual target token if it is. After each prediction, the total loss is increased by comparing the predicted token to the expected token using the loss function. Finally, the optimizers are updated using the results of this iteration of the model, and the average loss is returned."]},{"cell_type":"code","metadata":{"id":"3S9CM-2rzyuI","colab_type":"code","colab":{}},"source":["teacher_forcing_ratio = 0 # Make 0 if don't want teacher forcing\n","\n","# Training function\n","# Criterion = negative log likelihood loss (NLLLoss)\n","# Lines commented out are for attn decoder\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n","    encoder_hidden = encoder.init_hidden() # Initialize hidden state of the encoder\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","    input_length = input_tensor.size(0) # length of the input sequence\n","    target_length = target_tensor.size(0) # length of the target sequence\n","    loss = 0\n","\n","# loop through the input tokens w/ encoder and get the final vector/hidden state\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","\n","    decoder_input = target_tensor[0]\n","    decoder_hidden = encoder_hidden # Initialize the hidden state of the decoder\n","    # Decide whether to use teacher forcing on this run\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    # Run the decoder for each element of the target sequence\n","    if use_teacher_forcing:\n","      for di in range(target_length):\n","        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","        loss = loss + criterion(decoder_output, target_tensor[di])\n","        decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","      for di in range(target_length):\n","        #print(decoder_input)\n","        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","        decoder_input = decoder_output.squeeze()\n","        loss = loss + criterion(decoder_output, target_tensor[di].unsqueeze(0))\n","\n","    loss.backward()\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","    return loss.item() / target_length"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAyUrRN51edl","colab_type":"text"},"source":["# Training overhead\n","In order to train the model using an entire training dataset, an overhead function is required to oversee the process. This function takes the encoder and decoder as parameters, along with a few user-defined training variables including the number of iterations to train over, intervals at which to print and plot results, and the learning rate, which determines how much the optimizer will be influenced by each iteration.\n","\n","The function defines the optimizers for the encoder and decoder, takes a series of random input, target pairs, defines the criterion function, and runs the train function on a different pair for the number of iterations specified by the user. Periodically, progress is printed using the previously defined helper functions, so that the user can keep track of the model's performance. Additionally, \"snapshots\" of the model's optimized parameters are saved at the 20%, 40%, 55%, 70%, 85%, and 100% milestones in the training process so that these models can be evaluated individually and examined for overfitting and underfitting."]},{"cell_type":"code","metadata":{"id":"iaO-yHnJz3gU","colab_type":"code","colab":{}},"source":["# Repeatedly run the train function and print evaluation info as it goes\n","def train_iterations(encoder, decoder, n_iterations, print_every=1000, plot_every=100, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    # Get the training pairs\n","    training_pairs = [tensors_from_pair(random.choice(pairs)) for i in range(n_iterations)]\n","    criterion = nn.MSELoss()\n","\n","    # Loop to train the model with the specified number of iterations\n","    for iteration in range(1, n_iterations + 1):\n","        training_pair = training_pairs[iteration - 1]\n","        input_tensor = training_pair[0] # Get an input tensor from the pair\n","        target_tensor = training_pair[1]  # Get a target tensor from the pair\n","\n","        # Train the model on the pairs\n","        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        loss_total = loss_total + loss\n","        plot_loss_total = plot_loss_total + loss\n","\n","        if(iteration == 40000):\n","          torch.save(encoder.state_dict(), '/content/encoder40percent.dict')\n","          torch.save(decoder.state_dict(), '/content/decoder40percent.dict')\n","        \n","        if(iteration == 80000):\n","          torch.save(encoder.state_dict(), '/content/encoder80percent.dict')\n","          torch.save(decoder.state_dict(), '/content/decoder80percent.dict')\n","\n","        if(iteration == 100000):\n","          torch.save(encoder.state_dict(), '/content/encoder100percent.dict')\n","          torch.save(decoder.state_dict(), '/content/decoder100percent.dict')\n","\n","        # If it has reached the print interval, print progress information\n","        if iteration % print_every == 0:\n","            loss_avg = loss_total/print_every\n","            loss_total = 0\n","            print('%s (%d %d%%) Current Loss Value ----> %.4f' % (time_since(start, iteration / n_iterations), iteration, iteration / n_iterations * 100, loss_avg))\n","\n","        # If it has reached the plot interval, add info to the plot_losses array\n","        if iteration % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    print(\"=================================================================\")\n","    print(\"-------> MODEL HAS FINISHED TRAINING <-------\\n\")\n","    print(\"Plotting Model......\")\n","    show_plot(plot_losses)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oioATreT1fsf","colab_type":"text"},"source":["# Train the model\n","\n","Define a size for the hidden vector, initialize an encoder and decoder, then run the train_iterations function. In our implementation, data points with 25 input features are used to produce predictions with 25 output features."]},{"cell_type":"code","metadata":{"id":"SDjq678g0AG2","colab_type":"code","colab":{}},"source":["hidden_size = 256\n","encoder1 = Encoder(input_size=25,hidden_size=hidden_size).to(device)\n","decoder1 = Decoder(hidden_size=hidden_size,output_size=25).to(device)\n","\n","train_iterations(encoder1, decoder1, n_iterations=100000, print_every=100, plot_every=100, learning_rate=0.0001)"],"execution_count":0,"outputs":[]}]}