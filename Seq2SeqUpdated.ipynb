{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2SeqUpdated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jV6KTIm0_EU",
        "colab_type": "text"
      },
      "source": [
        "This first bit of code takes care of a few things. First, it gets some of the necessary imports out of the way and sets pandas print options. Then, it reads in the sequence csv file and filters out unnecessary columns, leaving 5 total features:\n",
        "\n",
        "1.   The position of the packet within its sequence. This is used only to parse the csv file back out into sequences.\n",
        "2.   The timestamp of the packet\n",
        "3.   The packet length\n",
        "4.   The source port\n",
        "5.   The destination port"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIrung1NzTb7",
        "colab_type": "code",
        "outputId": "b335d2ee-f81a-4298-8b91-b5517ab21182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import psutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Read the data and filter out unnecessary columns\n",
        "data = pd.read_csv(\"SequenceCSV.csv\", delimiter=',', header=0)\n",
        "#data = data.filter(items=['Position','Time','Source IP','Destination IP','Length','Source Port','Destination Port'])\n",
        "data = data.filter(items=['Position','Time','Length','Source Port','Destination Port'])\n",
        "print(data)\n",
        "#num_sequences = data.Position.value_counts()[0]\n",
        "#print(num_sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Position          Time  Length  Source Port  Destination Port\n",
            "0              0     37.294817      60           79              1024\n",
            "1              1     37.295017      60         1024                79\n",
            "2              2     37.295563      60         1024                79\n",
            "3              3     37.307251      60           79              1024\n",
            "4              4     37.327150      60           79              1024\n",
            "...          ...           ...     ...          ...               ...\n",
            "465100         3  25253.689350      60        15901                23\n",
            "465101         0  25253.710980      60         8803                23\n",
            "465102         1  25253.779420      60        15901                23\n",
            "465103         2  25253.850980      60         9959                23\n",
            "465104         3  25253.860990      60         8803                23\n",
            "\n",
            "[465105 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc5l3Xmm1Mbw",
        "colab_type": "text"
      },
      "source": [
        "# Feature vector encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmutqJy3zZpK",
        "colab_type": "code",
        "outputId": "ff3857bb-8056-4cba-fc4c-1384c6228c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# Encode a numeric feature\n",
        "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
        "    if mean is None:\n",
        "        mean = df[name].mean()\n",
        "\n",
        "    if sd is None:\n",
        "        sd = df[name].std()\n",
        "\n",
        "    df[name] = (df[name] - mean) / sd\n",
        "\n",
        "# Encode a categorical feature\n",
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = f\"{name}-{x}\"\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)\n",
        "\n",
        "encode_numeric_zscore(data, \"Time\")\n",
        "encode_numeric_zscore(data, \"Length\")\n",
        "encode_numeric_zscore(data, \"Source Port\")\n",
        "encode_numeric_zscore(data, \"Destination Port\")\n",
        "\n",
        "data.dropna(inplace=True,axis=1)\n",
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Position      Time    Length  Source Port  Destination Port\n",
            "0              0 -2.057470 -0.413749    -0.896562         -0.600173\n",
            "1              1 -2.057470 -0.413749    -0.802388         -0.698951\n",
            "2              2 -2.057470 -0.413749    -0.802388         -0.698951\n",
            "3              3 -2.057468 -0.413749    -0.896562         -0.600173\n",
            "4              4 -2.057465 -0.413749    -0.896562         -0.600173\n",
            "...          ...       ...       ...          ...               ...\n",
            "465100         3  1.951252 -0.413749     0.680176         -0.704804\n",
            "465101         0  1.951256 -0.413749    -0.027174         -0.704804\n",
            "465102         1  1.951267 -0.413749     0.680176         -0.704804\n",
            "465103         2  1.951278 -0.413749     0.088027         -0.704804\n",
            "465104         3  1.951280 -0.413749    -0.027174         -0.704804\n",
            "\n",
            "[465105 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZW7FBND1aWY",
        "colab_type": "text"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keIjrBrfzcIB",
        "colab_type": "code",
        "outputId": "2f6c4e7e-5bfa-41b9-b56f-ab745f82fce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Get the starting points of each sequence within the dataset\n",
        "points = []\n",
        "for x in range(0, len(data.index)):\n",
        "  if(data['Position'][x] == 0):\n",
        "    points.append(x)\n",
        "\n",
        "# Get the input and target sequences for the pairs\n",
        "pairs = []\n",
        "for x in range(0, len(points)-1):\n",
        "  count = 0\n",
        "  input = []\n",
        "  target = []\n",
        "  for y in range(points[x], points[x+1]):\n",
        "    if(count < 3):\n",
        "      # Get input sequence\n",
        "      packet = [data['Time'][y], data['Length'][y], data['Source Port'][y], data['Destination Port'][y]]\n",
        "      #packet = [data['Time'][y], data['Length'][y]]\n",
        "      input.append(packet)\n",
        "      count = count + 1\n",
        "    else:\n",
        "      # Get target sequence\n",
        "      packet = [data['Time'][y], data['Length'][y], data['Source Port'][y], data['Destination Port'][y]]\n",
        "      #packet = [data['Time'][y], data['Length'][y]]\n",
        "      target.append(packet)\n",
        "      count = count + 1\n",
        "\n",
        "  pair = [input, target]\n",
        "  pairs.append(pair)\n",
        "\n",
        "print(random.choice(pairs))\n",
        "print(len(pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1.325582279934195, -0.41374851003069607, -0.6929669550728281, -0.698846004774689], [1.3255827616219964, -0.41374851003069607, -0.8964619072795385, -0.4854034355178569], [1.3260805629131358, -0.41374851003069607, 0.8362348282826909, -0.7048040000967455]], [[1.3261155353551535, -0.41374851003069607, 0.8362348282826909, -0.7048040000967455]]]\n",
            "47355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U08BiNbs1a_o",
        "colab_type": "text"
      },
      "source": [
        "# Make the tensors\n",
        "\n",
        "The final step of data preparation requires us to transform the [input, target] pairs into tensors so that the model can read them. This is done simply with the following two functions:\n",
        "\n",
        "1.   tensor_from_sequence - converts the token vectors into tensors\n",
        "2.   tensors_from_pair - calls the previous function for the input and target sequences of a given pair and returns the resulting tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlsfTdVmzfkv",
        "colab_type": "code",
        "outputId": "f1c2f75d-a308-4915-c0fa-a8512d04afb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Functions to prepare the data for insertion into the model\n",
        "def tensor_from_sequence(sequence):  # Create a tensor from a sequence using the dictionary\n",
        "    return torch.tensor(sequence, dtype=torch.float, device=device)\n",
        "\n",
        "def tensors_from_pair(pair):  # Get an input and target tensor out of a pair\n",
        "    input_tensor = tensor_from_sequence(pair[0])\n",
        "    target_tensor = tensor_from_sequence(pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "  \n",
        "input_tensor, target_tensor = tensors_from_pair(pairs[42500])\n",
        "print(\"Input Tensor: \\n\", input_tensor)\n",
        "print(\"Target Tensor: \\n\", target_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Tensor: \n",
            " tensor([[ 1.5554, -0.4137,  0.6802, -0.7048],\n",
            "        [ 1.5554, -0.4137,  0.7599, -0.7049],\n",
            "        [ 1.5554, -0.4137,  0.7345, -0.6988]])\n",
            "Target Tensor: \n",
            " tensor([[ 1.5554, -0.4137, -0.8965,  1.0118]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ffMuxHG1blK",
        "colab_type": "text"
      },
      "source": [
        "The following few segments of code define the two parts of the sequence to sequence (seq2seq) model: the encoder and the decoder. Each of these two parts is a recurrent neural network (RNN), which is a neural network that performs some operation on a sequence of data and uses the output generated by that operation as input for the next step (recurrence). For these RNNs, we use the **Gated Recurrent Unit** (GRU) architecture, as opposed to the more commonly used **Long Short Term Memory** (LSTM) architecture. This is because, despite being a newer architecture, GRU works similarly to LSTM and has been shown to yield similar results while being slightly more efficient computationally. [This paper](https://arxiv.org/pdf/1412.3555v1.pdf) gives a more in-depth overview of the differences between the two architectures.\n",
        "# Encoder\n",
        "In a seq2seq model using an encoder and decoder, the responsibility of the encoder is to encode, or condense, the input sequence into a single vector while retaining the original meaning of that sequence. For each packet in the input sequence, the encoder will produce two things using the embedding layers:\n",
        "\n",
        "\n",
        "\n",
        "1.   A **vector** (called output_vector in the following code)\n",
        "2.   A **hidden state** (called hidden_state in the following code)\n",
        "\n",
        "\n",
        "\n",
        "Following this, a new input sequence and the previous hidden state will be taken as input to do the next step on the next packet in the sequence, and the output vector will be adjusted accordingly and a new hidden state produced. This process is repeated until a final output vector (the **context vector**) is reached, which will be given to the decoder later on. The forward function carries out these tasks in our implementation. \n",
        "\n",
        "This implementation is a bit different from a typical Encoder RNN, however, because it uses multiple features as input. Since there are multiple features, the size of the GRU must be multiplied by the number of total features, which is shown on line 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCf_629NzmAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recurrent neural network for Encoder of the seq2seq model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size, hidden_size) # Applies Gated Recurrent Unit (GRU) to input sequence\n",
        "\n",
        "    def forward(self, input_token, hidden_state):\n",
        "        input_token = input_token.unsqueeze(0).unsqueeze(0)\n",
        "        #print(input_token)\n",
        "        output_vector, hidden_state = self.gru(input_token, hidden_state)\n",
        "        return output_vector, hidden_state\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DUMFW1e1cIz",
        "colab_type": "text"
      },
      "source": [
        "# Decoder\n",
        "\n",
        "The decoder, like the encoder, is a recurrent neural network using GRU architecture. The decoder takes the context vector as its initial hidden state. As before, the forward function carries out the necessary steps, taking an input token and hidden state as input, then producing an output vector and new hidden state. Unlike the encoder, however, the decoder applies the softmax function to the output vector for normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRdRXOo-zpXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recurrent neural network for Decoder of seq2seq model\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(output_size, hidden_size) # Applies GRU\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_token, hidden_state):\n",
        "        input_token = input_token.unsqueeze(0).unsqueeze(0)\n",
        "        output_vector = self.relu(input_token)\n",
        "        #print(output_vector)\n",
        "        output_vector, hidden_state = self.gru(output_vector, hidden_state)\n",
        "        output_vector = self.out(output_vector[0])\n",
        "        return output_vector, hidden_state\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC65KhId1cut",
        "colab_type": "text"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhEs_wsKzs9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Helper functions to keep track of the time elapsed and time remaining\n",
        "def as_minutes(sec):\n",
        "    mins = math.floor(sec / 60)\n",
        "    sec = sec - (mins * 60)\n",
        "    return '%dm %ds' % (mins, sec)\n",
        "\n",
        "def time_since(since, percent):\n",
        "    now = time.time()\n",
        "    sec = now-since\n",
        "    es = sec/(percent)\n",
        "    rs = es-sec\n",
        "    return '%sec (- %sec)' % (as_minutes(sec), as_minutes(rs))\n",
        "\n",
        "# Plot loss vs number of iterations\n",
        "def show_plot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.5) # Put plot ticks at intervals of 0.5\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JutH17sC1dwT",
        "colab_type": "text"
      },
      "source": [
        "# Training function\n",
        "The training function performs the core aspect of machine learning model optimization. In this implementation, it takes many parameters:\n",
        "\n",
        "1. Input tensor which has the tensor representation of the first 3 packets in the sequence\n",
        "2. Target tensor\n",
        "3. The encoder and decoder that the function is going to be training\n",
        "4. Optimizers for the encoder and decoder to oversee the machine learning process\n",
        "5. A function (Negative log likelihood loss) to calculate the training loss, which is called \"criterion\" in the code\n",
        "6. The maximum length a sequence can have\n",
        "\n",
        "The function follows a process of several steps, beginning with preparation, which includes initializing the hidden state of the encoder, wiping the gradients of the optimizers, and getting the lengths of the input and target sequences. Next, we have to run the encoder on all of the tokens in the input sequence, which is handled in a loop. Once the encoder has produced an output vector, it is passed to the decoder along with the decoder's input token – the most recently predicted token if teacher forcing is not being used, and the actual target token if it is. After each prediction, the total loss is increased by comparing the predicted token to the expected target using the loss function. Finally, the optimizers are updated using the results of this iteration of the model, and the average loss is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S9CM-2rzyuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0 # Make 0 if don't want teacher forcing\n",
        "MAX_LENGTH = 320\n",
        "\n",
        "# Training function\n",
        "# Criterion = negative log likelihood loss (NLLLoss)\n",
        "# Lines commented out are for attn decoder\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.init_hidden() # Initialize hidden state of the encoder\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    input_length = input_tensor.size(0) # length of the input sequence\n",
        "    target_length = target_tensor.size(0) # length of the target sequence\n",
        "    loss = 0\n",
        "\n",
        "# loop through the input tokens w/ encoder and get the final vector/hidden state\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "\n",
        "    decoder_input = target_tensor[0]\n",
        "    decoder_hidden = encoder_hidden # Initialize the hidden state of the decoder\n",
        "    # Decide whether to use teacher forcing on this run\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Run the decoder for each element of the target sequence\n",
        "    if use_teacher_forcing:\n",
        "      for di in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        loss = loss + criterion(decoder_output, target_tensor[di])\n",
        "        decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "      for di in range(target_length):\n",
        "        #print(decoder_input)\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        decoder_input = decoder_output.squeeze()\n",
        "        loss = loss + criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAyUrRN51edl",
        "colab_type": "text"
      },
      "source": [
        "# Training overhead\n",
        "In order to train the model using an entire training dataset, an overhead function is required to oversee the process. This function takes the encoder and decoder as parameters, along with a few user-defined training variables including the number of iterations to train over, intervals at which to print and plot results, and the learning rate, which determines how much the optimizer will be influenced by each iteration.\n",
        "\n",
        "The function defines the optimizers for the encoder and decoder, takes a series of random input, target pairs, defines the criterion function, and runs the train function on a different pair for the number of iterations specified by the user. Periodically, progress is printed using the previously defined helper functions, so that the user can keep track of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaO-yHnJz3gU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repeatedly run the train function and print evaluation info as it goes\n",
        "def train_iterations(encoder, decoder, n_iterations, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    # Get the training pairs\n",
        "    training_pairs = [tensors_from_pair(random.choice(pairs)) for i in range(n_iterations)]\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Loop to train the model with the specified number of iterations\n",
        "    for iteration in range(1, n_iterations + 1):\n",
        "        training_pair = training_pairs[iteration - 1]\n",
        "        input_tensor = training_pair[0] # Get an input tensor from the pair\n",
        "        target_tensor = training_pair[1]  # Get a target tensor from the pair\n",
        "\n",
        "        # Train the model on the pairs\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        loss_total = loss_total + loss\n",
        "        plot_loss_total = plot_loss_total + loss\n",
        "\n",
        "        if(iteration == 40000):\n",
        "          torch.save(encoder.state_dict(), '/content/encoder20percent.dict')\n",
        "          torch.save(decoder.state_dict(), '/content/decoder20percent.dict')\n",
        "        \n",
        "        if(iteration == 80000):\n",
        "          torch.save(encoder.state_dict(), '/content/encoder40percent.dict')\n",
        "          torch.save(decoder.state_dict(), '/content/decoder40percent.dict')\n",
        "\n",
        "        if(iteration == 110000):\n",
        "          torch.save(encoder.state_dict(), '/content/encoder55percent.dict')\n",
        "          torch.save(decoder.state_dict(), '/content/decoder55percent.dict')\n",
        "\n",
        "        if(iteration == 140000):\n",
        "          torch.save(encoder.state_dict(), '/content/encoder70percent.dict')\n",
        "          torch.save(decoder.state_dict(), '/content/decoder70percent.dict')\n",
        "\n",
        "        if(iteration == 170000):\n",
        "          torch.save(encoder.state_dict(), '/content/encoder85percent.dict')\n",
        "          torch.save(decoder.state_dict(), '/content/decoder85percent.dict')\n",
        "\n",
        "        if(iteration == 200000):\n",
        "          torch.save(encoder.state_dict(), '/content/encoder100percent.dict')\n",
        "          torch.save(decoder.state_dict(), '/content/decoder100percent.dict')\n",
        "\n",
        "        # If it has reached the print interval, print progress information\n",
        "        if iteration % print_every == 0:\n",
        "            loss_avg = loss_total/print_every\n",
        "            loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (time_since(start, iteration / n_iterations), iteration, iteration / n_iterations * 100, loss_avg))\n",
        "\n",
        "        # If it has reached the plot interval, add info to the plot_losses array\n",
        "        if iteration % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    show_plot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oioATreT1fsf",
        "colab_type": "text"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "Define a size for the hidden vector, initialize an encoder and decoder, then run the train_iterations function.\n",
        "\n",
        "NOTE: SKIP IF TESTING A TRAINED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDjq678g0AG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = Encoder(input_size=4,hidden_size=hidden_size).to(device)\n",
        "decoder1 = Decoder(hidden_size=hidden_size,output_size=4).to(device)\n",
        "\n",
        "# originally had iterations at 40,000\n",
        "train_iterations(encoder1, decoder1, n_iterations=200000, print_every=1000, learning_rate=0.0002)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}